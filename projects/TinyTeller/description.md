# TinyTeller: A GPT-2 Children's Short Story Generator

This project implements **TinyTeller**, a fine-tuned version of **GPT-2 (small)** designed to generate simple short stories for children. It leverages the **TinyStories dataset**, which contains brief narratives aimed at young audiences, to fine-tune the model's language generation capabilities.

## Project Structure
The source code is organized into the following files within the `src` folder:

- **`TinyTeller.ipynb`**: This Jupyter notebook contains the core implementation for fine-tuning GPT-2 on the TinyStories dataset using Hugging Face's Transformers library. It includes code for preprocessing the dataset, training the model, and generating a short story.

- **`GPT-2_reference.ipynb`**: A reference notebook that provides code to run inference on the pre-trained GPT-2 model without fine-tuning. This serves as a baseline to evaluate the performance improvements achieved through fine-tuning with the TinyStories dataset.

## Model Architecture

TinyTeller is based on the GPT-2 (small) architecture. To optimize performance given hardware constraints, TinyTeller uses a reduced sequence length of **512 tokens**, compared to the original GPT-2â€™s sequence length of 1024 tokens. This reduction both conserves memory and aligns with the characteristics of the TinyStories dataset, which primarily consists of short stories. The dataset's average sequence length is approximately 222 tokens (median = 191, SD = 106), making 512 tokens a suitable choice for capturing complete narratives while keeping training manageable.

---

## Dataset and Preprocessing  

The fine-tuning was performed on the [TinyStories dataset](https://github.com/sri9s/tinystories-language-models.git), which contains about **2 million short stories** generated by GPT-3.5 and GPT-4. However, to accelerate the fine-tuning process within the limited hardware constraints, only a subset of approximately 200,000 stories (10% of the total dataset) was used for training.

---

## Experimental Results

The fine-tuning process was conducted for 3 epochs with a **physical batch size of 8** and **8 gradient accumulation steps**, resulting in a **total batch size of 64**. The training strategy also included the use of AdamW with a linear weight decay of 0.01 and a learning rate warm-up over 500 steps.

### Sample Stories 

Below are example generated stories from TinyTeller compared to the original GPT-2 without fine-tuning on the TinyStories dataset.

***

**Propt**: Once upon a time there was a pumpkin.

**TinyTeller**:
```
Once upon a time there was a pumpkin. It was very big and round, but it was also very cold. One day the sun shone down on the pumpkin and made it shiver. Suddenly, the pumpkin started to shiver because of its cold. 

The farmer came outside and saw the pumpkin shivering. He knew he had to do something to make it warmer. He took out his warm mittens and started to knit a sweater for the pumpkin.  The farmer worked hard all day and soon enough he finished the sweater.  
 
When the mittens were finished, the farmer put them on the pumpkin. When he went inside, he told everyone that the pumpkin looked so much better now. Everyone was so happy and excited! From then on they always kept their eyes open when playing near the pumpkin!

The end.
```

**GPT-2**:

```
Once upon a time there was a pumpkin. Then I saw it in the bushes. I was like, 'Oh, what the fuck?' Then this pumpkin came out, and it was big. It was 5 feet tall and had a big, big pumpkin on the top. And I said, Oh, that's the pumpkin, but I'm not sure where it came from, because I don't know, I mean, the whole pumpkin thing, it's like a little red brick. But it really was bigger than I thought it would be."

After he was fired from the department, she said the store's director told her that "you can't have a 'real pumpkin' without a fake one." The pumpkin was purchased at the Pumpkin Spice Factory in nearby Belmont, where she says she had "never seen a real pumpkin before." She said she would have been surprised if the fake pumpkin had been able to stay with the owner. and "I think, oh, we should have had this one in a different place because it looks like it had an apple in it," she explained. "But we didn't, so we took it out." Now, her employer, a local construction company, has been trying to get her to return to work.
```

***

**Propt**: Once upon a time there was a little girl named Lucy.

**TinyTeller**:
```
Once upon a time there was a little girl named Lucy. She loved to explore and play in the sun. One day, she saw something very special. It was a big bowl of delicious food! Lucy had never seen anything like it before and was very excited.

She wanted to try it all. So she asked her mom if she could have some. Her mom said yes and gave her a spoonful. But when Lucy tried it, she didn't like it very much. 

Lucy asked her dad why it tasted so bad. He told her that sometimes things that look good can be very bitter. Lucy was sad that she couldn't have some of the tasty food, but she still enjoyed it.
 
So Lucy went back outside and played with other children. They were all enjoying their time together in the sunshine. The sun was shining brightly, making everything beautiful. Lucy smiled and thanked them for being kind to her.

The End. 
```

**GPT-2**:
```
Once upon a time there was a little girl named Lucy. She was one of the few children to survive the storm and was brought to the shelter of a small shelter where she was adopted by a group of people. They named her Lucy and gave her a name.

The family had a lot of issues with Lucy, but they were able to talk and see the way she looked. The family and Lucy were happy to see each other again. Lucy was now a beautiful young woman who is now doing well. Some of her friends, however, were afraid of Lucy's appearance and had to be taken to a shelter. One of them took her to another shelter and took care of some of those who were suffering. Then they gave Lucy a new name, Lucy (the name of an old girl who had died in the hurricane).
, a very special girl. Her name is Lucy from the song "The Bell" by the band A&R. There are many of these songs and there are a million of others. In the next section we will look at a few of my favorite songs of all time.
```
***

**Propt**: Once upon a time, there lived a black cat.

**TinyTeller**
```
Once upon a time, there lived a black cat. The cat liked to climb. Every day the cat would climb over logs and rocks. One day, as the cat was climbing around a corner of its house it saw something shiny. It had a big metal box with a lock on it! The cat wanted to know what was inside the box. 

So the cat decided that it could open the lid of the box and see what was there. When the cat opened the lid, out came lots of colorful balls. They were so light and sparkly! The cats were very excited.

The cat then started playing with the balls. Every time the cat rolled them around in circles, they lit up in a beautiful way. The cats played until the sun went down. Then when it got dark the cats climbed back into their homes, happy and tired from all the fun they had.

The End. 
```


**GPT-2**:
```
Once upon a time, there lived a black cat. The cat was a beast of burden, and the black lion was an animal of need. It was the cat of the future, the beast who was not afraid of death and death's destruction.


This black-hued cat, a cat that was born out of fear of losing its body, was called the Beast of Death. He was said to be the Black Lion, but the lion's name was The Beast. These were his words. "The Beast is the one who knows the answer to all questions."...
```

---

## Discussion 

The outputs from TinyTeller demonstrate its ability to generate coherent and engaging short stories with simple, accessible language suitable for young children. In contrast, GPT-2 often produces less coherent text, straying off-topic or generating nonsensical narratives. This underscores the effectiveness of fine-tuning on the TinyStories dataset, which enables the model to produce higher-quality short stories.

However, some limitations were found. For example, the model still lacks storytelling quality in terms of plot and narrative structure. Additionally, it occasionally fails to properly conclude a story, continuing to generate content even after the story has already ended.

---
