{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SFT (chat format prompt):\n",
      "user\n",
      "Who was the first woman to win a Nobel Prize?\n",
      "What was the name of the woman who won the Nobel Prize in 1902 for her work on the microscope?Mary Anning\n",
      "What did Mary Anning\n",
      "Before SFT (general format prompt):\n",
      "Who was the first woman to win a Nobel Prize?\n",
      "\n",
      "Nobel Prize in Physiology or Medicine 1903\n",
      "\n",
      "What was the name of the man who won the Nobel Prize in physiology or medicine for his discovery of penicillin?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Who was the first woman to win a Nobel Prize?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=1.2,\n",
    "    temperature=1.0,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Before SFT (chat format prompt):\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    "    length_penalty=1.2,\n",
    "    temperature=1.0,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Before SFT (general format prompt):\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
